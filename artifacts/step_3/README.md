# 3 спринт. Артефакты

Напомним артефакты:

- Датасет, разделённый на трейн и тест;
- Анализ метрик рекомендательных систем.

Проделана следующая работа:

1. Для следующего спринта готовы данные, разделённые на трейн и тест - они (и остальные части текущего датасета) загружены в S3. Помимо этого, предоставлен скрипт для разделения ([ссылка](https://github.com/TheoLisin/recsys_MADE/tree/ml/src/data_splitting)). В качестве стратегии разделения выбран индуктивный подход - полученный граф разделён на компоненты связности, которые разнесены на трейн и тест.
2. Проведён анализ метрик рекомендательных систем - отчёт доступен в репозитории ([ссылка](https://github.com/TheoLisin/recsys_MADE/blob/ml/artifacts/step_3/metrics.pdf)). Результат - на этапе Evaluation будем использовать `Jaccard similarity`, на этапе проверки качества можно будет использовать `MAP@k` и/или `coverage`/`diversity`.

Также в конце предыдущего спринта упоминалось о дополнительной предобработке - она сделана, датасет (предобработанный содержит ~1м статей) лежит в S3. Помимо этого, доступны ноутбуки с процессом работы и объяснениями ([ссылка на папку в `nbviewer`](https://nbviewer.org/github/TheoLisin/recsys_MADE/tree/ml/src/data_extraction/), [ссылка на папку в репо](https://github.com/TheoLisin/recsys_MADE/tree/ml/src/data_extraction)).


Q & A:
Q: "на основе чего выбиралось кол-во топиков/кластеров при построении моделей?" 
A: Для финального классификатора используется `Human in the loop ML` подход, который подразумевает разметку человеком для придания "логичности" названию темы. Это сильно ограничивает максимальное количество кластеров (`BERTopic`, например, генерирует порядка 200 кластеров), которые можно сгенерировать. При этом это количество не должно быть слишком маленьким, чтобы сохранялся "простор" в выборе темы статьи. В итоге остановились на 30, поскольку для `ZeroShotTM`, по нашим оценкам, разница в качестве выдачи в пределах 25-40 топиков не сильно большая (топики получаются достаточно качественными).